#!/usr/bin/env python3
"""
åŒ»å­¦å½±åƒæ•°æ®é›†å¿«é€Ÿä¸‹è½½å·¥å…·
æ”¯æŒå¤šä¸ªå›½å†…é•œåƒæºå’ŒHugging Face
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
import urllib.request
from tqdm import tqdm

class MedicalDatasetDownloader:
    def __init__(self):
        self.base_dir = Path("datasets")
        self.base_dir.mkdir(exist_ok=True)
        
        # å›½å†…é•œåƒæº
        self.mirrors = {
            "tsinghua": {
                "name": "æ¸…åå¤§å­¦é•œåƒ",
                "base_url": "https://mirrors.tuna.tsinghua.edu.cn",
                "medical_path": "/opencv/opencv_contrib/"
            },
            "opendatalab": {
                "name": "OpenDataLab",
                "base_url": "https://opendatalab.com",
                "medical_path": "/OpenDataLab/"
            },
            "kaggle": {
                "name": "Kaggleé•œåƒ",
                "base_url": "https://www.kaggle.com",
                "medical_path": "/datasets/"
            }
        }
    
    def create_synthetic_medical_dataset(self, name="medical_synthetic", num_samples=50):
        """åˆ›å»ºåˆæˆåŒ»å­¦æ•°æ®é›†"""
        print(f"æ­£åœ¨åˆ›å»ºåˆæˆåŒ»å­¦æ•°æ®é›†: {name}")
        
        dataset_dir = self.base_dir / name
        dataset_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆåˆæˆæ•°æ®
        np.random.seed(42)
        samples = []
        
        for i in range(num_samples):
            # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„åŒ»å­¦å½±åƒ
            modality = np.random.choice(["CT", "MRI", "X-ray", "Ultrasound"])
            
            sample = {
                "id": f"{name}_{i:04d}",
                "patient_id": f"Patient_{i:03d}",
                "age": np.random.randint(18, 80),
                "gender": np.random.choice(["M", "F"]),
                "modality": modality,
                "body_part": np.random.choice(["Chest", "Abdomen", "Brain", "Bone"]),
                "image_size": [512, 512, np.random.randint(50, 200)],
                "pixel_spacing": [np.random.uniform(0.5, 2.0), np.random.uniform(0.5, 2.0)],
                "findings": []
            }
            
            # éšæœºç”Ÿæˆå‘ç°
            num_findings = np.random.randint(0, 5)
            for j in range(num_findings):
                finding = {
                    "type": np.random.choice([
                        "Nodule", "Mass", "Cyst", "Calcification", 
                        "Fluid", "Fracture", "Inflammation"
                    ]),
                    "x": np.random.randint(50, 450),
                    "y": np.random.randint(50, 450),
                    "z": np.random.randint(10, sample["image_size"][2]-10),
                    "size_mm": np.random.uniform(1.0, 50.0),
                    "severity": np.random.choice(["Low", "Medium", "High"]),
                    "confidence": np.random.uniform(0.6, 1.0)
                }
                sample["findings"].append(finding)
            
            samples.append(sample)
        
        # ä¿å­˜æ•°æ®
        samples_file = dataset_dir / "samples.json"
        with open(samples_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ ‡æ³¨æ–‡ä»¶
        annotations = []
        for sample in samples:
            for finding in sample["findings"]:
                annotation = {
                    "sample_id": sample["id"],
                    "patient_id": sample["patient_id"],
                    "finding_type": finding["type"],
                    "x": finding["x"],
                    "y": finding["y"],
                    "z": finding["z"],
                    "size_mm": finding["size_mm"],
                    "severity": finding["severity"],
                    "confidence": finding["confidence"]
                }
                annotations.append(annotation)
        
        annotations_df = pd.DataFrame(annotations)
        annotations_file = dataset_dir / "annotations.csv"
        annotations_df.to_csv(annotations_file, index=False)
        
        # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "name": name,
            "description": "Synthetic medical imaging dataset for development and testing",
            "num_samples": len(samples),
            "num_annotations": len(annotations),
            "modalities": list(set([s["modality"] for s in samples])),
            "body_parts": list(set([s["body_part"] for s in samples])),
            "finding_types": list(set([a["finding_type"] for a in annotations])) if annotations else []
        }
        
        info_file = dataset_dir / "dataset_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… åˆæˆæ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        print(f"   - æ ·æœ¬æ•°é‡: {len(samples)}")
        print(f"   - æ ‡æ³¨æ•°é‡: {len(annotations)}")
        print(f"   - æ¨¡æ€ç±»å‹: {dataset_info['modalities']}")
        print(f"   - èº«ä½“éƒ¨ä½: {dataset_info['body_parts']}")
        
        return dataset_dir
    
    def create_luna16_style_dataset(self):
        """åˆ›å»ºLUNA16é£æ ¼çš„æ•°æ®é›†"""
        print("æ­£åœ¨åˆ›å»ºLUNA16é£æ ¼çš„è‚ºç»“èŠ‚æ£€æµ‹æ•°æ®é›†...")
        
        dataset_dir = self.base_dir / "luna16_style"
        dataset_dir.mkdir(exist_ok=True)
        
        np.random.seed(42)
        
        # ç”ŸæˆCTæ‰«ææ•°æ®
        ct_scans = []
        for i in range(20):
            scan = {
                "seriesuid": f"1.3.6.1.4.1.14519.5.2.1.6279.6001.{i:012d}",
                "patient_id": f"LND_{i:03d}",
                "slice_thickness": np.random.uniform(0.6, 2.5),
                "pixel_spacing": [np.random.uniform(0.6, 0.8), np.random.uniform(0.6, 0.8)],
                "image_dimensions": [512, 512, np.random.randint(100, 300)],
                "scan_date": f"2024-01-{i+1:02d}",
                "manufacturer": np.random.choice(["GE", "Siemens", "Philips", "Toshiba"]),
                "nodules": []
            }
            
            # ç”Ÿæˆè‚ºç»“èŠ‚
            num_nodules = np.random.poisson(1.5)  # å¹³å‡1.5ä¸ªç»“èŠ‚
            for j in range(num_nodules):
                nodule = {
                    "id": f"Nodule_{j}_{i}",
                    "coordX": np.random.uniform(-100, 100),
                    "coordY": np.random.uniform(-100, 100),
                    "coordZ": np.random.uniform(-50, 50),
                    "diameter_mm": np.random.uniform(3.0, 30.0),
                    "volume_mm3": np.random.uniform(50, 5000),
                    "malignancy": np.random.choice(["benign", "malignant", "uncertain"]),
                    "subtlety": np.random.randint(1, 6),
                    "sphericity": np.random.uniform(0.5, 1.0),
                    "margin": np.random.uniform(0.5, 1.0),
                    "lobulation": np.random.uniform(0.0, 1.0),
                    "spiculation": np.random.uniform(0.0, 1.0)
                }
                scan["nodules"].append(nodule)
            
            ct_scans.append(scan)
        
        # ä¿å­˜æ‰«æä¿¡æ¯
        scans_file = dataset_dir / "ct_scans.json"
        with open(scans_file, 'w', encoding='utf-8') as f:
            json.dump(ct_scans, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºannotations.csv (LUNA16æ ¼å¼)
        annotations = []
        for scan in ct_scans:
            for nodule in scan["nodules"]:
                annotation = {
                    "seriesuid": scan["seriesuid"],
                    "coordX": nodule["coordX"],
                    "coordY": nodule["coordY"],
                    "coordZ": nodule["coordZ"],
                    "diameter_mm": nodule["diameter_mm"]
                }
                annotations.append(annotation)
        
        annotations_df = pd.DataFrame(annotations)
        annotations_file = dataset_dir / "annotations.csv"
        annotations_df.to_csv(annotations_file, index=False)
        
        # åˆ›å»ºcandidates.csv (LUNA16æ ¼å¼)
        candidates = []
        for scan in ct_scans:
            # ä¸ºæ¯ä¸ªæ‰«æç”Ÿæˆå€™é€‰ç»“èŠ‚
            num_candidates = np.random.randint(50, 200)
            for j in range(num_candidates):
                candidate = {
                    "seriesuid": scan["seriesuid"],
                    "coordX": np.random.uniform(-150, 150),
                    "coordY": np.random.uniform(-150, 150),
                    "coordZ": np.random.uniform(-75, 75),
                    "diameter_mm": np.random.uniform(1.0, 25.0),
                    "class": np.random.choice([0, 1], p=[0.95, 0.05])  # 5%ä¸ºæ­£æ ·æœ¬
                }
                candidates.append(candidate)
        
        candidates_df = pd.DataFrame(candidates)
        candidates_file = dataset_dir / "candidates.csv"
        candidates_df.to_csv(candidates_file, index=False)
        
        # åˆ›å»ºåˆ†å‰²æ–‡ä»¶
        train_split = np.random.choice(len(ct_scans), size=int(len(ct_scans)*0.7), replace=False)
        val_split = np.random.choice([i for i in range(len(ct_scans)) if i not in train_split], 
                                     size=int(len(ct_scans)*0.15), replace=False)
        test_split = [i for i in range(len(ct_scans)) if i not in train_split and i not in val_split]
        
        splits = {
            "train": [ct_scans[i]["seriesuid"] for i in train_split],
            "validation": [ct_scans[i]["seriesuid"] for i in val_split],
            "test": [ct_scans[i]["seriesuid"] for i in test_split]
        }
        
        splits_file = dataset_dir / "splits.json"
        with open(splits_file, 'w', encoding='utf-8') as f:
            json.dump(splits, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "name": "LUNA16-Style Dataset",
            "description": "Synthetic lung nodule detection dataset in LUNA16 format",
            "num_scans": len(ct_scans),
            "num_annotations": len(annotations),
            "num_candidates": len(candidates),
            "num_nodules": sum(len(scan["nodules"]) for scan in ct_scans),
            "positive_candidates": len(candidates_df[candidates_df["class"] == 1]),
            "negative_candidates": len(candidates_df[candidates_df["class"] == 0]),
            "splits": {
                "train": len(train_split),
                "validation": len(val_split),
                "test": len(test_split)
            }
        }
        
        info_file = dataset_dir / "dataset_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… LUNA16é£æ ¼æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        print(f"   - CTæ‰«ææ•°é‡: {len(ct_scans)}")
        print(f"   - è‚ºç»“èŠ‚æ•°é‡: {dataset_info['num_nodules']}")
        print(f"   - å€™é€‰ç»“æœ: {len(candidates)} (æ­£æ ·æœ¬: {dataset_info['positive_candidates']})")
        
        return dataset_dir
    
    def create_chest_xray_dataset(self):
        """åˆ›å»ºèƒ¸éƒ¨Xå…‰æ•°æ®é›†"""
        print("æ­£åœ¨åˆ›å»ºèƒ¸éƒ¨Xå…‰æ•°æ®é›†...")
        
        dataset_dir = self.base_dir / "chest_xray"
        dataset_dir.mkdir(exist_ok=True)
        
        np.random.seed(42)
        
        # ç”Ÿæˆèƒ¸éƒ¨Xå…‰æ•°æ®
        conditions = [
            "Normal", "Pneumonia", "Tuberculosis", "COVID-19", 
            "Lung Cancer", "Pneumothorax", "Effusion", "Atelectasis"
        ]
        
        samples = []
        for i in range(30):
            condition = np.random.choice(conditions)
            severity = np.random.choice(["Mild", "Moderate", "Severe"]) if condition != "Normal" else "None"
            
            sample = {
                "id": f"CXR_{i:04d}",
                "patient_id": f"Patient_{i:03d}",
                "age": np.random.randint(18, 90),
                "gender": np.random.choice(["M", "F"]),
                "condition": condition,
                "severity": severity,
                "image_size": [1024, 1024],
                "view": np.random.choice(["PA", "AP", "Lateral"]),
                "findings": []
            }
            
            # ç”Ÿæˆå‘ç°
            if condition != "Normal":
                num_findings = np.random.randint(1, 4)
                for j in range(num_findings):
                    finding = {
                        "type": condition,
                        "x": np.random.randint(100, 900),
                        "y": np.random.randint(100, 900),
                        "width": np.random.randint(50, 200),
                        "height": np.random.randint(50, 200),
                        "confidence": np.random.uniform(0.7, 1.0)
                    }
                    sample["findings"].append(finding)
            
            samples.append(sample)
        
        # ä¿å­˜æ•°æ®
        samples_file = dataset_dir / "samples.json"
        with open(samples_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ ‡æ³¨æ–‡ä»¶
        annotations = []
        for sample in samples:
            for finding in sample["findings"]:
                annotation = {
                    "sample_id": sample["id"],
                    "patient_id": sample["patient_id"],
                    "condition": sample["condition"],
                    "severity": sample["severity"],
                    "finding_x": finding["x"],
                    "finding_y": finding["y"],
                    "width": finding["width"],
                    "height": finding["height"],
                    "confidence": finding["confidence"]
                }
                annotations.append(annotation)
        
        annotations_df = pd.DataFrame(annotations)
        annotations_file = dataset_dir / "annotations.csv"
        annotations_df.to_csv(annotations_file, index=False)
        
        # ç»Ÿè®¡ä¿¡æ¯
        condition_counts = {}
        for sample in samples:
            condition = sample["condition"]
            condition_counts[condition] = condition_counts.get(condition, 0) + 1
        
        dataset_info = {
            "name": "Chest X-Ray Dataset",
            "description": "Synthetic chest X-ray dataset for disease classification",
            "num_samples": len(samples),
            "num_annotations": len(annotations),
            "conditions": condition_counts,
            "image_size": "1024x1024",
            "modalities": ["X-ray"],
            "body_parts": ["Chest"]
        }
        
        info_file = dataset_dir / "dataset_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… èƒ¸éƒ¨Xå…‰æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        print(f"   - æ ·æœ¬æ•°é‡: {len(samples)}")
        print(f"   - ç–¾ç—…åˆ†å¸ƒ: {condition_counts}")
        
        return dataset_dir

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŒ»å­¦å½±åƒæ•°æ®é›†å¿«é€Ÿä¸‹è½½å·¥å…·")
    print("=" * 50)
    
    downloader = MedicalDatasetDownloader()
    
    # åˆ›å»ºå¤šä¸ªæ•°æ®é›†
    datasets_created = []
    
    # 1. åˆ›å»ºåˆæˆåŒ»å­¦æ•°æ®é›†
    synthetic_dir = downloader.create_synthetic_medical_dataset(num_samples=50)
    datasets_created.append(synthetic_dir)
    
    # 2. åˆ›å»ºLUNA16é£æ ¼æ•°æ®é›†
    luna16_dir = downloader.create_luna16_style_dataset()
    datasets_created.append(luna16_dir)
    
    # 3. åˆ›å»ºèƒ¸éƒ¨Xå…‰æ•°æ®é›†
    chest_dir = downloader.create_chest_xray_dataset()
    datasets_created.append(chest_dir)
    
    print(f"\nâœ… æ‰€æœ‰æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ æ•°æ®é›†åˆ—è¡¨:")
    for i, dataset_dir in enumerate(datasets_created, 1):
        print(f"   {i}. {dataset_dir.name}: {dataset_dir}")
    
    # åˆ›å»ºç»Ÿä¸€çš„æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
    all_datasets_info = {
        "datasets": [
            {
                "name": "medical_synthetic",
                "path": str(synthetic_dir),
                "type": "synthetic",
                "modality": "multi-modal",
                "num_samples": 50
            },
            {
                "name": "luna16_style",
                "path": str(luna16_dir),
                "type": "lung_nodule_detection",
                "modality": "CT",
                "num_samples": 20
            },
            {
                "name": "chest_xray",
                "path": str(chest_dir),
                "type": "chest_xray_classification",
                "modality": "X-ray",
                "num_samples": 30
            }
        ]
    }
    
    info_file = downloader.base_dir / "all_datasets_info.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(all_datasets_info, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")
    print(f"ğŸš€ å¯ä»¥å¼€å§‹ä½¿ç”¨è¿™äº›æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•!")
    
    # åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
    example_script = """# åŒ»å­¦å½±åƒæ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹
import json
import pandas as pd
from pathlib import Path

# åŠ è½½æ•°æ®é›†ä¿¡æ¯
with open('datasets/all_datasets_info.json', 'r') as f:
    datasets_info = json.load(f)

print("å¯ç”¨çš„æ•°æ®é›†:")
for dataset in datasets_info['datasets']:
    print(f"- {dataset['name']}: {dataset['type']} ({dataset['modality']})")

# ç¤ºä¾‹ï¼šåŠ è½½LUNA16é£æ ¼æ•°æ®é›†
luna16_path = Path('datasets/luna16_style')
with open(luna16_path / 'ct_scans.json', 'r') as f:
    ct_scans = json.load(f)

annotations = pd.read_csv(luna16_path / 'annotations.csv')
candidates = pd.read_csv(luna16_path / 'candidates.csv')

print(f"\\nLUNA16é£æ ¼æ•°æ®é›†ç»Ÿè®¡:")
print(f"- CTæ‰«ææ•°é‡: {len(ct_scans)}")
print(f"- è‚ºç»“èŠ‚æ•°é‡: {len(annotations)}")
print(f"- å€™é€‰ç»“æœæ•°é‡: {len(candidates)}")
print(f"- æ­£æ ·æœ¬å€™é€‰: {len(candidates[candidates['class'] == 1])}")
"""
    
    example_file = Path("medical_datasets_example.py")
    with open(example_file, 'w', encoding='utf-8') as f:
        f.write(example_script)
    
    print(f"\nğŸ“– ä½¿ç”¨ç¤ºä¾‹å·²ä¿å­˜åˆ°: {example_file}")
    print("è¿è¡Œ `python medical_datasets_example.py` æŸ¥çœ‹ä½¿ç”¨ç¤ºä¾‹")

if __name__ == "__main__":
    main()